# Usage:
#   CPU mode:  docker compose up -d --build
#   GPU mode:  docker compose --env-file .env --env-file .env.gpu --profile gpu up -d --build
#
# See also: make up / make up-gpu

services:
  postgres:
    image: postgres:18.2-trixie
    restart: unless-stopped
    environment:
      POSTGRES_DB: labeling
      POSTGRES_USER: labeling
      POSTGRES_PASSWORD: ${DB_PASSWORD:-labeling}
    volumes:
      - postgres_data:/var/lib/postgresql
      - ./saegim-backend/migrations/001_init.sql:/docker-entrypoint-initdb.d/001_init.sql:ro
    ports:
      - "${PG_PORT:-15432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U labeling -d labeling"]
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 10s
    networks:
      - saegim-net

  backend:
    build:
      context: ./saegim-backend
      dockerfile: Dockerfile
      target: ${BUILD_TARGET:-production}
      args:
        BASE_IMAGE: ${BASE_IMAGE:-ubuntu:noble}
        TORCH_EXTRA: ${TORCH_EXTRA:-cpu}
    restart: unless-stopped
    environment:
      DATABASE_URL: postgresql://labeling:${DB_PASSWORD:-labeling}@postgres:5432/labeling
      API_HOST: "0.0.0.0"
      API_PORT: "5000"
      STORAGE_PATH: /workspace/storage
      CORS_ORIGINS: '["http://localhost:${FE_PORT:-13000}"]'
      LOG_LEVEL: INFO
    volumes:
      - ./storage:/workspace/storage
    ports:
      - "${API_PORT:-15000}:5000"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5000/api/v1/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - saegim-net

  frontend:
    build:
      context: ./saegim-frontend
      args:
        VITE_API_URL: ${VITE_API_URL:-http://localhost:15000}
    restart: unless-stopped
    ports:
      - "${FE_PORT:-13000}:80"
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - saegim-net

  # Optional: vLLM server for Chandra OCR
  # Enable with: docker compose --profile gpu up
  vllm:
    image: vllm/vllm-openai:v0.15.1
    profiles:
      - gpu
    restart: unless-stopped
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      HF_HOME: /root/.cache/huggingface
      VLLM_ATTENTION_BACKEND: TORCH_SDPA
    command: >
      --model ${VLLM_MODEL:-richarddavison/chandra-fp8}
      --gpu-memory-utilization ${VLLM_GPU_UTIL:-0.9}
      --max-num-seqs ${VLLM_MAX_SEQS:-4}
      --max-model-len ${VLLM_MAX_MODEL_LEN:-32768}
      --max-num-batched-token ${VLLM_MAX_BATCHED_TOKENS:-65536}
    ports:
      - "${VLLM_PORT:-18000}:8000"
    ipc: host
    volumes:
      - ${HF_CACHE_DIR:-vllm_cache}:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 120s
    networks:
      - saegim-net

volumes:
  postgres_data:
  vllm_cache:

networks:
  saegim-net:
    driver: bridge
